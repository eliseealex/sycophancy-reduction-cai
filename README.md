# Reduction of Sycophantic Behaviour with Constitutional AI

This project is **in progress**.

In the scope of this project I plan to fine-tune an open source model (Mistral-7B) to evaluate, how well Constitutional AI fine-tuning could be used to reduce sycophancy in LLMs.

This research is based on two other papers:
1. [Simple synthetic data reduces sycophancy in large language models](https://arxiv.org/abs/2308.03958) by Google Deep Mind.
2. [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) by Anthropic.

## Background

This is a project done in the scope of [AI Safety Fundamentals Alignment Course](https://course.aisafetyfundamentals.com/alignment). My main motivation is to gain technical skills of fine-tuning and doing empirical AI Alignment research, though, I picked a project that may help the AI Safety community to get some new interesting observations about the behaviour of LLMs.

## License, Copyright Notice and Changes

This project includes material that is derived from [evals](https://github.com/anthropics/evals/tree/main) by Anthropic which is licensed under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/). Changes were made to this material by Aleksandr Eliseev (@eliseealex).

This project includes material that is derived from [sycophancy-intervention](https://github.com/google/sycophancy-intervention?tab=readme-ov-file) by Google which is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0). Changes were made to this material by Aleksandr Eliseev (@eliseealex).
